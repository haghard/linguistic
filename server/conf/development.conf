hostname = "127.0.0.1"
hostname = ${?HOSTNAME}
akkaport = 2551
akkaport = ${?AKKA_PORT}


akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 30s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    allow-java-serialization = off

    serializers {
      ser = "linguistic.serializers.LinguisticsSerializer"
    }

    serialization-bindings {
      "linguistic.protocol.package$WordsQuery" = ser
      "linguistic.protocol.package$HomophonesQuery" = ser
      "linguistic.protocol.package$Words" = ser
      "linguistic.protocol.package$Homophones" = ser
      "linguistic.protocol.package$Homophone" = ser
      "linguistic.protocol.package$SearchResults" = ser
    }

  }

  remote {
    log-remote-lifecycle-events = off

    netty.tcp {
      hostname = ${hostname}
      port = 0
    }
  }

  http {

    session {
      max-age = 1200 s
      server-secret = "nadf84yhfgasdhvasdh8q74y5q78ehdpaSKPDFKLDPGIJ9E56OPSDFJGALJW3HRA89WEHFGSDKBNALD;9RJYAERHDLFHMDL;FMHEJ5R96WJRHME0RT9JKYW409J5ALD"
      server-secret = ${?SERVER_SECRET}
    }

    interface = ${?hostname}

    dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-factor = 2.0
        parallelism-max = 6
      }
      throughput = 1000
    }

    blocking-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 4
      }
      throughput = 100
    }

    ssl {
      keypass = avmiejtq
      storepass = akdfopjb
    }
  }

  cluster {
    #auto-down-unreachable-after = 10 seconds

    # MajorityLeaderAutoDowning is similar to QuorumLeaderAutoDowning. However, instead of a static specified quorum size
    # this strategy automatically keeps the partition with the largest amount of nodes. If the partitions are of equal size,
    # the partition that contains the node with the globally lowest address is kept. The strategy is the same as the keep majority
    # strategy of Split Brain Resolver from Typesafe reactive platform. If a role is set by majority-member-role,
    # the strategy is only enforced to the nodes with the specified role.
    downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"


    roles = [ "linguistic-engine" ]

    #min-nr-of-members = 1
    log-info = on
    use-dispatcher = "cluster-dispatcher"

    # CoordinatedShutdown will run the tasks that are added to these
    # phases. The phases can be ordered as a DAG by defining the
    # dependencies between the phases.
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # depends-on=[]: Run the phase after the given phases
    coordinated-shutdown {
      # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
      # if this is set to 'on'. It is done after termination of the
      # ActorSystem if terminate-actor-system=on, otherwise it is done
      # immediately when the last phase is reached.
      exit-jvm = on
      default-phase-timeout = 10 seconds
    }

    # https://doc.akka.io/docs/akka/2.5.5/scala/cluster-sharding.html
    sharding {

      buffer-size = 1000

      role = "linguistic-engine"

      #Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 120 s

      journal-plugin-id = "cassandra-journal"
      snapshot-plugin-id = "cassandra-snapshot-store"

      # Rebalance check is performed periodically with this interval
      rebalance-interval = 30 s

      snapshot-after = 7200
      waiting-for-state-timeout = 5 s
      updating-state-timeout = 5 s
      use-dispatcher = shard-dispatcher

      #remember-entities = on # default: off

      state-store-mode  = persistence # default: ddata
    }
  }

  akka.extensions = [ akka.cluster.metrics.ClusterMetricsExtension ]

  persistence {
    journal {
      max-message-batch-size = 200
      max-confirmation-batch-size = 10000
      max-deletion-batch-size = 10000
      plugin = "cassandra-journal"
    }
    snapshot-store {
      plugin = "cassandra-snapshot-store"
    }
  }
}

cassandra-journal {

  #cluster-id = "linguistics_cluster"

  #local-datacenter = "west"

  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Comma-separated list of contact points in the cluster
  #contact-points = ["138.68.93.181","192.168.77.42"]

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the journal
  keyspace = "linguistics"

  # Name of the table to be created/used by the journal
  table = "linguistics_journal"

  # Replication factor to use when creating a keyspace
  replication-factor = 2

  #data-center-replication-factors = ["west:2", "east:2"]

  # Write consistency level
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of entries per partition (= columns per row).
  # Must not be changed after table creation (currently not checked).
  max-partition-size = 5000000

  # Maximum size of result set
  max-result-size = 50001

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"

  # Dispatcher for fetching and replaying messages
  replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"

  # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
  # false to avoid the overhead of maintaining the tag_views table.
  events-by-tag.enabled = false
}

cassandra-snapshot-store {

  #cluster-id = "linguistics_cluster"

  #local-datacenter = "west"

  # FQCN of the cassandra snapshot store plugin
  class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Parameter indicating whether the journal keyspace should be auto created
  keyspace-autocreate = true

  # Parameter indicating whether the journal tables should be auto created
  tables-autocreate = true

  # Comma-separated list of contact points in the cluster
  #contact-points = ["138.68.93.181","192.168.77.42"]

  # Port of contact points in the cluster
  port = 9042

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "linguistics"

  # Name of the table to be created/used by the snapshot store
  table = "linguistics_snapshot"

  # Replication factor to use when creating a keyspace
  replication-factor = 2

  #data-center-replication-factors = ["west:2", "east:2"]

  # Write consistency level
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of snapshot metadata to load per recursion (when trying to
  # find a snapshot that matches specified selection criteria). Only increase
  # this value when selection criteria frequently select snapshots that are
  # much older than the most recent snapshot i.e. if there are much more than
  # 10 snapshots between the most recent one and selected one. This setting is
  # only for increasing load efficiency of snapshots.
  max-metadata-result-size = 10

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "cassandra-snapshot-store.default-dispatcher"

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }
}

cluster-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 10
  }
}

shard-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 6
  }
  throughput = 1000
}