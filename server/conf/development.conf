hostname = "127.0.0.1"
hostname = ${?HOSTNAME}
akkaport = 2551
akkaport = ${?AKKA_PORT}


akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 30s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    allow-java-serialization = off

    serializers {
      ser = "linguistic.serializers.LinguisticsSerializer"
    }

    serialization-bindings {
      "linguistic.protocol.package$SearchQuery$WordsQuery" = ser
      "linguistic.protocol.package$SearchQuery$HomophonesQuery" = ser
      "linguistic.protocol.package$UniqueTermsByShard" = ser
      "linguistic.protocol.package$UniqueTermsByShard2" = ser
      "linguistic.protocol.package$Homophones" = ser
      "linguistic.protocol.package$Homophone" = ser
      "linguistic.protocol.package$SearchResults" = ser
      "linguistic.protocol.package$AddOneWord" = ser
      "linguistic.protocol.package$OneWordAdded" = ser
    }
  }

  remote {
    log-remote-lifecycle-events = off

    artery {
      # Select the underlying transport implementation.
      # Possible values: aeron-udp, tcp, tls-tcp
      # See https://doc.akka.io/docs/akka/current/remoting-artery.html#selecting-a-transport for the tradeoffs
      # for each transport

      #https://doc.akka.io/docs/akka/current/remoting-artery.html#selecting-a-transport
      #transport = aeron-udp
    }
  }

  http {

    session {
      max-age = 36000 s
      server-secret = "nadf84yhfgasdhvasdh8q74y5q78ehdpaSKPDFKLDPGIJ9E56OPSDFJGALJW3HRA89WEHFGSDKBNALD;9RJYAERHDLFHMDL;FMHEJ5R96WJRHME0RT9JKYW409J5ALD"
      server-secret = ${?SERVER_SECRET}
    }

    interface = ${?hostname}

    dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-factor = 2.0
        parallelism-max = 6
      }
      throughput = 1000
    }

    blocking-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 4
      }
      throughput = 100
    }

    ssl {
      keypass = avmiejtq
      storepass = akdfopjb
    }
  }

  cluster {
    #auto-down-unreachable-after = 10 seconds

    # MajorityLeaderAutoDowning is similar to QuorumLeaderAutoDowning. However, instead of a static specified quorum size
    # this strategy automatically keeps the partition with the largest amount of nodes. If the partitions are of equal size,
    # the partition that contains the node with the globally lowest address is kept. The strategy is the same as the keep majority
    # strategy of Split Brain Resolver from Typesafe reactive platform. If a role is set by majority-member-role,
    # the strategy is only enforced to the nodes with the specified role.
    #downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    split-brain-resolver {
      # static-quorum, keep-majority, keep-oldest, down-all, lease-majority


      # Keep the part that can acquire the lease, and down the other part.
      # Best effort is to keep the side that has most nodes, i.e. the majority side.
      # This is achieved by adding a delay before trying to acquire the lease on the
      # minority side.

      active-strategy = keep-majority

      #//#stable-after
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7s

      # When reachability observations by the failure detector are changed the SBR decisions
      # are deferred until there are no changes within the 'stable-after' duration.
      # If this continues for too long it might be an indication of an unstable system/network
      # and it could result in delayed or conflicting decisions on separate sides of a network
      # partition.
      # As a precaution for that scenario all nodes are downed if no decision is made within
      # `stable-after + down-all-when-unstable` from the first unreachability event.
      # The measurement is reset if all unreachable have been healed, downed or removed, or
      # if there are no changes within `stable-after * 2`.
      # The value can be on, off, or a duration.
      # By default it is 'on' and then it is derived to be 3/4 of stable-after.
      down-all-when-unstable = on
    }

    #min-nr-of-members = 1
    log-info = on
    use-dispatcher = "cluster-dispatcher"

    # CoordinatedShutdown will run the tasks that are added to these
    # phases. The phases can be ordered as a DAG by defining the
    # dependencies between the phases.
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # depends-on=[]: Run the phase after the given phases
    coordinated-shutdown {

      phases {
        cluster-exiting {
          timeout = 10 s
          depends-on = [cluster-leave]
        }

        cluster-sharding-shutdown-region {
          timeout = 10 s
          depends-on = [before-cluster-shutdown]
        }
      }

      exit-jvm = on
      default-phase-timeout = 5 seconds
    }

    # https://doc.akka.io/docs/akka/2.5.5/scala/cluster-sharding.html
    sharding {

      buffer-size = 1000

      #role = "linguistic-engine"

      #Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # How often the coordinator saves persistent snapshots, which are
      # used to reduce recovery times
      snapshot-interval = 120 s

      # Rebalance check is performed periodically with this interval
      rebalance-interval = 30 s

      snapshot-after = 7200

      # Timeout of waiting the initial distributed state for the shard coordinator (an initial state will be queried again if the timeout happened)
      # and for a shard to get its state when remembered entities is enabled
      # The read from ddata is a ReadMajority, for small clusters (< majority-min-cap) every node needs to respond
      # so is more likely to time out if there are nodes restarting e.g. when there is a rolling re-deploy happening
      waiting-for-state-timeout = 10 s


      # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
      # Also used as timeout for writes of remember entities when that is enabled
      updating-state-timeout = 12 s

      use-dispatcher = shard-dispatcher

      #disables
      remember-entities = on # default: off
      # Defines how the coordinator stores its state. remember-entities = on enables it globally for all shards
      # Same is also used by the shards for rememberEntities.
      # Valid values are "ddata" or "persistence". "persistence" mode is deprecated

      #journal-plugin-id = "cassandra-journal"
      #snapshot-plugin-id = "cassandra-snapshot-store"
      state-store-mode = persistence  # default: ddata
    }
  }

  akka.extensions = [ akka.cluster.metrics.ClusterMetricsExtension ]

  persistence {

    # When starting many persistent actors at the same time the journal
    # and its data store is protected from being overloaded by limiting number
    # of recoveries that can be in progress at the same time. When
    # exceeding the limit the actors will wait until other recoveries have
    # been completed.
    max-concurrent-recoveries = 6 #50

    //    journal {
    //      plugin = "akka.persistence.journal.leveldb"
    //      auto-start-journals = ["akka.persistence.journal.leveldb"]
    //    }
    //
    //    snapshot-store {
    //      plugin = "akka.persistence.snapshot-store.local"
    //      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
    //    }


    #https://doc.akka.io/docs/akka/current/typed/persistence.html#replay-filter
    #https://blog.softwaremill.com/akka-cluster-split-brain-failures-are-you-ready-for-it-d9406b97e099
    journal-plugin-fallback.replay-filter {
      # What the filter should do when detecting invalid events.
      # Supported values:
      # `repair-by-discard-old` : discard events from old writers,
      #                           warning is logged
      # `fail` : fail the replay, error is logged
      # `warn` : log warning but emit events untouched
      # `off`  : disable this feature completely
      mode = repair-by-discard-old

      window-size = 100
    }

    journal {
      plugin = "akka.persistence.cassandra.journal"
      auto-start-journals = [akka.persistence.cassandra.journal]
    }

    snapshot-store {
      plugin = "akka.persistence.cassandra.snapshot"
      auto-start-snapshot-stores = [akka.persistence.cassandra.snapshot]
    }

    cassandra {

      journal {

        plugin-dispatcher = shard-dispatcher #akka.actor.internal-dispatcher

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "linguistics"
        table = "linguistics_journal"

        # Maximum number of messages that will be batched when using `persistAsync`.
        # Also used as the max batch size for deletes.
        max-message-batch-size = 200

        # Target number of entries per partition (= columns per row).
        # Must not be changed after table creation (currently not checked).
        # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
        target-partition-size = 500000

        replication-factor = 1 #3

        support-all-persistence-ids = off
      }

      query {
        plugin-dispatcher = shard-dispatcher

        # New events are retrieved (polled) with this interval.
        refresh-interval = 50 millis #3s

        # How many events to fetch in one query (replay) and keep buffered until they
        # are delivered downstreams.
        max-buffer-size = 500
      }

      events-by-tag {
        # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
        # false to avoid the overhead of maintaining the tag_views table.
        enabled = false

        # https://doc.akka.io/docs/akka-persistence-cassandra/current/events-by-tag.html#back-tracking
        back-track {
          # Interval at which events by tag stages trigger a query to look for delayed events from before the
          # current offset. Delayed events are also searched for when a subsequent event for the same persistence id
          # is received. The purpose of this back track is to find delayed events for persistence ids that are no
          # longer receiving events. Depending on the size of the period of the back track this can be expensive.
          interval = 1s

          # How far back to go in time for the scan. This is the maxinum amount of time an event can be delayed
          # and be found without receiving a new event for the same persistence id. Set to a duration or "max" to search back
          # to the start of the previous bucket which is the furthest a search can go back.
          period = 5s

          # at a less frequent interval for a longer back track is done to find any events that were delayed significantly
          long-interval = 120s

          # long-period can be max, off, or a duration where max is to the start of the previous bucket or cleanup-old-persistence-ids,
          # which ever is shorter. Back tracks can not be longer than the cleanup-old-persistence-ids otherwise old events
          # will be redelivered due to the metadat having been dropped
          long-period = "max"
        }
      }

      snapshot {

        # don't do this in production, convenient for local example
        keyspace-autocreate = true
        tables-autocreate = true

        keyspace = "linguistics"
        table = "linguistics_snapshots_journal"

        replication-factor = 1 #3
      }
    }
  }
}


datastax-java-driver {

  advanced {
    reconnect-on-init = true
    #auth-provider {
    #class = PlainTextAuthProvider
    #username = ...
    #password = ...
    #}
  }

  basic {
    contact-points = ["127.0.0.1:9042"]
    load-balancing-policy.local-datacenter = dc1
  }

  profiles {
    akka-persistence-cassandra-profile {
      basic.request {
        #ONE only for development
        consistency = ONE
      }
    }

    akka-persistence-cassandra-snapshot-profile {
      basic.request {
        #ONE only for development
        consistency = ONE
      }
    }
  }
}



cluster-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 10
  }
}

shard-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 8
  }
  throughput = 1
}

akka.management.cluster.bootstrap.contact-point-discovery {
  service-name = linguistic
  discovery-method = config

  # boostrap filters ports with the same IP assuming they are previous instances running on the same node
  # unless a port is specified
  port-name = "management"
  required-contact-point-nr = 1
  # config service discovery never changes
  stable-margin = 3 ms
  # bootstrap without all the nodes being up
  contact-with-all-contact-points = false
}

akka.discovery.config.services {
  "linguistic" {
    endpoints = [
      {host = "127.0.0.1", port = 8558}
      {host = "127.0.0.2", port = 8558}
    ]
  }
}

akka.extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]